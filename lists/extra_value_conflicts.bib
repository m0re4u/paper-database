@article{hoover2020moral,
  author   = {Joe Hoover and Gwenyth Portillo-Wightman and Leigh Yeh and Shreya Havaldar and Aida Mostafazadeh Davani and Ying Lin and Brendan Kennedy and Mohammad Atari and Zahra Kamel and Madelyn Mendlen and Gabriela Moreno and Christina Park and Tingyee E. Chang and Jenna Chin and Christian Leong and Jun Yen Leung and Arineh Mirinjian and Morteza Dehghani},
  title    = {Moral Foundations Twitter Corpus: A Collection of 35k Tweets Annotated for Moral Sentiment},
  journal  = {Social Psychological and Personality Science},
  volume   = {11},
  number   = {8},
  pages    = {1057-1071},
  year     = {2020},
  doi      = {10.1177/1948550619876629},
  url      = {https://doi.org/10.1177/1948550619876629},
  eprint   = {https://doi.org/10.1177/1948550619876629},
  abstract = { Research has shown that accounting for moral sentiment in natural language can yield insight into a variety of on- and off-line phenomena such as message diffusion, protest dynamics, and social distancing. However, measuring moral sentiment in natural language is challenging, and the difficulty of this task is exacerbated by the limited availability of annotated data. To address this issue, we introduce the Moral Foundations Twitter Corpus, a collection of 35,108 tweets that have been curated from seven distinct domains of discourse and hand annotated by at least three trained annotators for 10 categories of moral sentiment. To facilitate investigations of annotator response dynamics, we also provide psychological and demographic metadata for each annotator. Finally, we report moral sentiment classification baselines for this corpus using a range of popular methodologies. }
}


@inproceedings{alshomary2022moral,
  title     = {The Moral Debater: A Study on the Computational Generation of Morally Framed Arguments},
  author    = {Alshomary, Milad  and
               El Baff, Roxanne  and
               Gurcke, Timon  and
               Wachsmuth, Henning},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = may,
  year      = {2022},
  address   = {Dublin, Ireland},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2022.acl-long.601},
  doi       = {10.18653/v1/2022.acl-long.601},
  pages     = {8782--8797},
  abstract  = {An audience{'}s prior beliefs and morals are strong indicators of how likely they will be affected by a given argument. Utilizing such knowledge can help focus on shared values to bring disagreeing parties towards agreement. In argumentation technology, however, this is barely exploited so far. This paper studies the feasibility of automatically generating morally framed arguments as well as their effect on different audiences. Following the moral foundation theory, we propose a system that effectively generates arguments focusing on different morals. In an in-depth user study, we ask liberals and conservatives to evaluate the impact of these arguments. Our results suggest that, particularly when prior beliefs are challenged, an audience becomes more affected by morally framed arguments.}
}

@incollection{graham2013moral,
  title     = {Moral Foundations Theory: The Pragmatic Validity of Moral Pluralism},
  editor    = {Patricia Devine and Ashby Plant},
  booktitle = {Advances in Experimental Social Psychology},
  publisher = {Academic Press},
  volume    = {47},
  pages     = {55-130},
  year      = {2013},
  issn      = {0065-2601},
  doi       = {https://doi.org/10.1016/B978-0-12-407236-7.00002-4},
  url       = {https://www.sciencedirect.com/science/article/pii/B9780124072367000024},
  author    = {Jesse Graham and Jonathan Haidt and Sena Koleva and Matt Motyl and Ravi Iyer and Sean P. Wojcik and Peter H. Ditto},
  keywords  = {Morality, Nativism, Cultural learning, Intuition, Pluralism, Method-theory coevolution},
  abstract  = {Where does morality come from? Why are moral judgments often so similar across cultures, yet sometimes so variable? Is morality one thing, or many? Moral Foundations Theory (MFT) was created to answer these questions. In this chapter, we describe the origins, assumptions, and current conceptualization of the theory and detail the empirical findings that MFT has made possible, both within social psychology and beyond. Looking toward the future, we embrace several critiques of the theory and specify five criteria for determining what should be considered a foundation of human morality. Finally, we suggest a variety of future directions for MFT and moral psychology.}
}


@inproceedings{kobbe2020exploring,
  title     = {Exploring Morality in Argumentation},
  author    = {Kobbe, Jonathan  and
               Rehbein, Ines  and
               Hulpu{\textcommabelow{s}}, Ioana  and
               Stuckenschmidt, Heiner},
  booktitle = {Proceedings of the 7th Workshop on Argument Mining},
  month     = dec,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.argmining-1.4},
  pages     = {30--40},
  abstract  = {Sentiment and stance are two important concepts for the analysis of arguments. We propose to add another perspective to the analysis, namely moral sentiment. We argue that moral values are crucial for ideological debates and can thus add useful information for argument mining. In the paper, we present different models for automatically predicting moral sentiment in debates and evaluate them on a manually annotated testset. We then apply our models to investigate how moral values in arguments relate to argument quality, stance and audience reactions.}
}

@article{aldayel2021stance,
  title    = {Stance detection on social media: State of the art and trends},
  journal  = {Information Processing \& Management},
  volume   = {58},
  number   = {4},
  pages    = {102597},
  year     = {2021},
  issn     = {0306-4573},
  doi      = {https://doi.org/10.1016/j.ipm.2021.102597},
  url      = {https://www.sciencedirect.com/science/article/pii/S0306457321000960},
  author   = {Abeer ALDayel and Walid Magdy},
  keywords = {Stance detection, Stance, Social media, Stance classification},
  abstract = {Stance detection on social media is an emerging opinion mining paradigm for various social and political applications in which sentiment analysis may be sub-optimal. There has been a growing research interest for developing effective methods for stance detection methods varying among multiple communities including natural language processing, web science, and social computing, where each modeled stance detection in different ways. In this paper, we survey the work on stance detection across those communities and present an exhaustive review of stance detection techniques on social media, including the task definition, different types of targets in stance detection, features set used, and various machine learning approaches applied. Our survey reports state-of-the-art results on the existing benchmark datasets on stance detection, and discusses the most effective approaches. In addition, we explore the emerging trends and different applications of stance detection on social media, including opinion mining and prediction and recently using it for fake news detection. The study concludes by discussing the gaps in the current existing research and highlights the possible future directions for stance detection on social media.}
}



@inproceedings{hardalov2021cross,
  title     = {Cross-Domain Label-Adaptive Stance Detection},
  author    = {Hardalov, Momchil  and
               Arora, Arnav  and
               Nakov, Preslav  and
               Augenstein, Isabelle},
  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  month     = nov,
  year      = {2021},
  address   = {Online and Punta Cana, Dominican Republic},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2021.emnlp-main.710},
  doi       = {10.18653/v1/2021.emnlp-main.710},
  pages     = {9011--9028},
  abstract  = {Stance detection concerns the classification of a writer{'}s viewpoint towards a target. There are different task variants, e.g., stance of a tweet vs. a full article, or stance with respect to a claim vs. an (implicit) topic. Moreover, task definitions vary, which includes the label inventory, the data collection, and the annotation protocol. All these aspects hinder cross-domain studies, as they require changes to standard domain adaptation approaches. In this paper, we perform an in-depth analysis of 16 stance detection datasets, and we explore the possibility for cross-domain learning from them. Moreover, we propose an end-to-end unsupervised framework for out-of-domain prediction of unseen, user-defined labels. In particular, we combine domain adaptation techniques such as mixture of experts and domain-adversarial training with label embeddings, and we demonstrate sizable performance gains over strong baselines, both (i) in-domain, i.e., for seen targets, and (ii) out-of-domain, i.e., for unseen targets. Finally, we perform an exhaustive analysis of the cross-domain results, and we highlight the important factors influencing the model performance.}
}


@inproceedings{allaway2020zero,
  title     = {{Z}ero-{S}hot {S}tance {D}etection: {A} {D}ataset and {M}odel using {G}eneralized {T}opic {R}epresentations},
  author    = {Allaway, Emily  and
               McKeown, Kathleen},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  month     = nov,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.emnlp-main.717},
  doi       = {10.18653/v1/2020.emnlp-main.717},
  pages     = {8913--8931},
  abstract  = {Stance detection is an important component of understanding hidden influences in everyday life. Since there are thousands of potential topics to take a stance on, most with little to no training data, we focus on zero-shot stance detection: classifying stance from no training examples. In this paper, we present a new dataset for zero-shot stance detection that captures a wider range of topics and lexical variation than in previous datasets. Additionally, we propose a new model for stance detection that implicitly captures relationships between topics using generalized topic representations and show that this model improves performance on a number of challenging linguistic phenomena.}
}


@inproceedings{sogaard2021we,
  title     = {We Need To Talk About Random Splits},
  author    = {S{\o}gaard, Anders  and
               Ebert, Sebastian  and
               Bastings, Jasmijn  and
               Filippova, Katja},
  booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
  month     = apr,
  year      = {2021},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2021.eacl-main.156},
  doi       = {10.18653/v1/2021.eacl-main.156},
  pages     = {1823--1832},
  abstract  = {(CITATION) argued for using random splits rather than standard splits in NLP experiments. We argue that random splits, like standard splits, lead to overly optimistic performance estimates. We can also split data in biased or adversarial ways, e.g., training on short sentences and evaluating on long ones. Biased sampling has been used in domain adaptation to simulate real-world drift; this is known as the covariate shift assumption. In NLP, however, even worst-case splits, maximizing bias, often under-estimate the error observed on new samples of in-domain data, i.e., the data that models should minimally generalize to at test time. This invalidates the covariate shift assumption. Instead of using multiple random splits, future benchmarks should ideally include multiple, independent test sets instead; if infeasible, we argue that multiple biased splits leads to more realistic performance estimates than multiple random splits.}
}

@inproceedings{liu2021enhancing,
  title     = {Enhancing Zero-shot and Few-shot Stance Detection with Commonsense Knowledge Graph},
  author    = {Liu, Rui  and
               Lin, Zheng  and
               Tan, Yutong  and
               Wang, Weiping},
  booktitle = {Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
  month     = aug,
  year      = {2021},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2021.findings-acl.278},
  doi       = {10.18653/v1/2021.findings-acl.278},
  pages     = {3152--3157}
}


@inbook{friedman2013value,
  author    = {Friedman, Batya
               and Kahn, Peter H.
               and Borning, Alan
               and Huldtgren, Alina},
  title     = {Value Sensitive Design and Information Systems},
  booktitle = {Early engagement and new technologies: Opening up the laboratory},
  year      = {2013},
  publisher = {Springer Netherlands},
  address   = {Dordrecht},
  pages     = {55--95},
  abstract  = {Value Sensitive Design is a theoretically grounded approach to the design of technology that accounts for human values in a principled and comprehensive manner throughout the design process. It employs an integrative and iterative tripartite methodology, consisting of conceptual, empirical, and technical investigations. We explicate Value Sensitive Design by drawing on three case studies. The first study concerns information and control of web browser cookies, implicating the value of informed consent. The second study concerns using high-definition plasma displays in an office environment to provide a ``window'' to the outside world, implicating the values of physical and psychological well-being and privacy in public spaces. The third study concerns an integrated land use, transportation, and environmental simulation system to support public deliberation and debate on major land use and transportation decisions, implicating the values of fairness, accountability, and support for the democratic process, as well as a highly diverse range of values that might be held by different stakeholders, such as environmental sustainability, opportunities for business expansion, or walkable neighborhoods. We conclude with direct and practical suggestions for how to engage in Value Sensitive Design.},
  isbn      = {978-94-007-7844-3},
  doi       = {10.1007/978-94-007-7844-3_4},
  url       = {https://doi.org/10.1007/978-94-007-7844-3_4}
}

@inproceedings{obie2021first,
  author    = {Obie, Humphrey O. and Hussain, Waqar and Xia, Xin and Grundy, John and Li, Li and Turhan, Burak and Whittle, Jon and Shahin, Mojtaba},
  booktitle = {2021 IEEE/ACM 43rd International Conference on Software Engineering: Software Engineering in Society (ICSE-SEIS)},
  title     = {A First Look at Human Values-Violation in App Reviews},
  year      = {2021},
  volume    = {},
  number    = {},
  pages     = {29-38},
  doi       = {10.1109/ICSE-SEIS52602.2021.00012}
}


@inproceedings{chen2014understanding,
  author    = {Chen, Jilin and Hsieh, Gary and Mahmud, Jalal U. and Nichols, Jeffrey},
  title     = {Understanding Individuals' Personal Values from Social Media Word Use},
  year      = {2014},
  isbn      = {9781450325400},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2531602.2531608},
  doi       = {10.1145/2531602.2531608},
  abstract  = {The theory of values posits that each person has a set of values, or desirable and trans-situational goals, that motivate their actions. The Basic Human Values, a motivational construct that captures people's values, have been shown to influence a wide range of human behaviors. In this work, we analyze people's values and their word use on Reddit, an online social news sharing community. Through conducting surveys and analyzing text contributions of 799 Reddit users, we identify and interpret categories of words that are indicative of user's value orientations. Using the same data, we further report a preliminary exploration on word-based prediction of Basic Human Values.},
  booktitle = {Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work \& Social Computing},
  pages     = {405–414},
  numpages  = {10},
  keywords  = {word use, social media, basic human value},
  location  = {Baltimore, Maryland, USA},
  series    = {CSCW '14}
}


@article{bail2018exposure,
  author  = {Christopher A. Bail  and Lisa P. Argyle  and Taylor W. Brown  and John P. Bumpus  and Haohan Chen  and M. B. Fallin Hunzaker  and Jaemin Lee  and Marcus Mann  and Friedolin Merhout  and Alexander Volfovsky },
  title   = {Exposure to opposing views on social media can increase political polarization},
  journal = {Proceedings of the National Academy of Sciences},
  volume  = {115},
  number  = {37},
  pages   = {9216-9221},
  year    = {2018},
  doi     = {10.1073/pnas.1804840115},
  url     = {https://www.pnas.org/doi/abs/10.1073/pnas.1804840115},
  eprint  = {https://www.pnas.org/doi/pdf/10.1073/pnas.1804840115}
}

@article{boyd2015values,
  title   = {Values in Words: Using Language to Evaluate and Understand Personal Values},
  volume  = {9},
  url     = {https://ojs.aaai.org/index.php/ICWSM/article/view/14589},
  doi     = {10.1609/icwsm.v9i1.14589},
  number  = {1},
  journal = {Proceedings of the International AAAI Conference on Web and Social Media},
  author  = {Boyd,
             Ryan and Wilson,
             Steven and Pennebaker,
             James and Kosinski,
             Michal and Stillwell,
             David and Mihalcea,
             Rada},
  year    = {2021},
  month   = {Aug.},
  pages   = {31-40}
}


@article{beel2022linguistic,
  title   = {Linguistic Characterization of Divisive Topics Online: Case Studies on Contentiousness in Abortion,
             Climate Change,
             and Gun Control},
  volume  = {16},
  url     = {https://ojs.aaai.org/index.php/ICWSM/article/view/19270},
  doi     = {10.1609/icwsm.v16i1.19270},
  number  = {1},
  journal = {Proceedings of the International AAAI Conference on Web and Social Media},
  author  = {Beel,
             Jacob and Xiang,
             Tong and Soni,
             Sandeep and Yang,
             Diyi},
  year    = {2022},
  month   = {May},
  pages   = {32-42}
}

@article{bouman2018measuring,
  author   = {Bouman, Thijs and Steg, Linda and Kiers, Henk A. L.},
  title    = {Measuring Values in Environmental Research: A Test of an Environmental Portrait Value Questionnaire},
  journal  = {Frontiers in Psychology},
  volume   = {9},
  year     = {2018},
  url      = {https://www.frontiersin.org/articles/10.3389/fpsyg.2018.00564},
  doi      = {10.3389/fpsyg.2018.00564},
  issn     = {1664-1078},
  abstract = {Four human values are considered to underlie individuals’ environmental beliefs and behaviors: biospheric (i.e., concern for environment), altruistic (i.e., concern for others), egoistic (i.e., concern for personal resources) and hedonic values (i.e., concern for pleasure and comfort). These values are typically measured with an adapted and shortened version of the Schwartz Value Survey (SVS), to which we refer as the Environmental-SVS (E-SVS). Despite being well-validated, recent research has indicated some concerns about the SVS methodology (e.g., comprehensibility, self-presentation biases) and suggested an alternative method of measuring human values: The Portrait Value Questionnaire (PVQ). However, the PVQ has not yet been adapted and applied to measure values most relevant to understand environmental beliefs and behaviors. Therefore, we tested the Environmental-PVQ (E-PVQ) – a PVQ variant of E-SVS –and compared it with the E-SVS in two studies. Our findings provide strong support for the validity and reliability of both the E-SVS and E-PVQ. In addition, we find that respondents slightly preferred the E-PVQ over the E-SVS (Study 1). In general, both scales correlate similarly to environmental self-identity (Study 1), energy behaviors (Studies 1 and 2), pro-environmental personal norms, climate change beliefs and policy support (Study 2). Accordingly, both methodologies show highly similar results and seem well-suited for measuring human values underlying environmental behaviors and beliefs.}
}

@incollection{polletta2018forms,
  author    = {Polletta, Francesca and Gardner, Beth},
  isbn      = {9780198747369},
  title     = {{The Forms of Deliberative Communication}},
  booktitle = {{The Oxford Handbook of Deliberative Democracy}},
  publisher = {Oxford University Press},
  year      = {2018},
  month     = {09},
  abstract  = {{As deliberative democratic theory has moved from a macro theory of democratic legitimacy to prescriptions for institutional design, questions about what constitutes deliberative communication have taken on increasing practical importance. At the same time, empirical data has accumulated to answer those questions. We review findings on the kinds of talk that produce either mutually-agreed upon decisions or better understanding of the issues at stake, equality among speakers, and impacts on policies or participants after the forum is over. Deliberative talk in facilitated settings today does not resemble the abstract, dispassionate reason-giving imagined by many theorists of deliberation. However, precisely for that reason, deliberative talk today is producing some of the benefits claimed for it.}},
  doi       = {10.1093/oxfordhb/9780198747369.013.45},
  url       = {https://doi.org/10.1093/oxfordhb/9780198747369.013.45},
  eprint    = {https://academic.oup.com/book/0/chapter/212131829/chapter-ag-pdf/44595650/book\_28086\_section\_212131829.ag.pdf}
}

@article{cabitza2023toward,
  title   = {Toward a Perspectivist Turn in Ground Truthing for Predictive Computing},
  volume  = {37},
  url     = {https://ojs.aaai.org/index.php/AAAI/article/view/25840},
  doi     = {10.1609/aaai.v37i6.25840},
  number  = {6},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  author  = {Cabitza,
             Federico and Campagner,
             Andrea and Basile,
             Valerio},
  year    = {2023},
  month   = {Jun.},
  pages   = {6860-6868}
}


@inproceedings{gu2021package,
  title     = {A Package for Learning on Tabular and Text Data with Transformers},
  author    = {Gu, Ken  and
               Budhkar, Akshay},
  booktitle = {Proceedings of the Third Workshop on Multimodal Artificial Intelligence},
  month     = jun,
  year      = {2021},
  address   = {Mexico City, Mexico},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2021.maiworkshop-1.10},
  doi       = {10.18653/v1/2021.maiworkshop-1.10},
  pages     = {69--73},
  abstract  = {Recent progress in natural language processing has led to Transformer architectures becoming the predominant model used for natural language tasks. However, in many real- world datasets, additional modalities are included which the Transformer does not directly leverage. We present Multimodal- Toolkit, an open-source Python package to incorporate text and tabular (categorical and numerical) data with Transformers for downstream applications. Our toolkit integrates well with Hugging Face{'}s existing API such as tokenization and the model hub which allows easy download of different pre-trained models.}
}

@inproceedings{devlin2019bert,
  title     = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author    = {Devlin, Jacob  and
               Chang, Ming-Wei  and
               Lee, Kenton  and
               Toutanova, Kristina},
  booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  month     = jun,
  year      = {2019},
  address   = {Minneapolis, Minnesota},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/N19-1423},
  doi       = {10.18653/v1/N19-1423},
  pages     = {4171--4186},
  abstract  = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).}
}

@article{esau2022creates,
  volume    = {18},
  publisher = {University of Westminster Press},
  pages     = {1--16},
  month     = {June},
  title     = {What creates listening online? Exploring reciprocity in online political discussions with relational content analysis},
  number    = {1},
  year      = {2022},
  journal   = {Journal of Deliberative Democracy},
  author    = {K Esau and Dennis Friess},
  doi       = {10.16997/jdd.1021},
  abstract  = {{\ensuremath{<}}p{\ensuremath{>}}Democratic governments frequently use online tools to include large numbers of citizens in participation processes. Against the backdrop of deliberative theories, such initiatives are subject to normative needs. This article examines the equality of participation. Previous research has mainly focused on equality in terms of access to, and voice within, deliberation processes. However, much less is known about the factors that influence the distribution of reciprocity in online political discussions. Proposing a theoretical distinction between simple replying and deliberative reciprocity, this study addresses the question: What obstructs or promotes deliberative reciprocity online? Drawing on previous online communication research, we assume that communication style, gender and users{'} activity are important predictors of simple replying and deliberative reciprocity. Results of a quantitative relational content analysis indicate that in order to receive deliberative reciprocity users should ask questions, propose arguments, be humorous, have a critical attitude and use a male user name. Storytelling and expressions of emotions show no significant associations with deliberative reciprocity.{\ensuremath{<}}/p{\ensuremath{>}}},
  url       = {https://eprints.qut.edu.au/235147/}
}


@article{stromer2020context,
  title     = {Context and medium matter: Expressing disagreements online and face-to-face in political deliberations},
  author    = {Stromer-Galley, Jennifer and Bryant, Lauren and Bimber, Bruce},
  journal   = {Journal of Deliberative Democracy},
  volume    = {11},
  doi       = {https://doi.org/10.16997/jdd.218},
  number    = {1},
  year      = {2020},
  publisher = {University of Westminster Press}
}

@article{lin2022learning,
  title    = {Learning from disagreement on social media: The mediating role of like-minded and cross-cutting discussion and the moderating role of fact-checking},
  journal  = {Computers in Human Behavior},
  volume   = {139},
  pages    = {107558},
  year     = {2023},
  issn     = {0747-5632},
  doi      = {https://doi.org/10.1016/j.chb.2022.107558},
  url      = {https://www.sciencedirect.com/science/article/pii/S0747563222003788},
  author   = {Han Lin and Yonghwan Kim},
  keywords = {Social media news, Learning from disagreement, Fact-checking, Like-minded discussion, Cross-cutting discussion},
  abstract = {Social networking sites (SNSs) have the potential to be effective in facilitating exposure to disagreement. However, whether exposure to disagreement leads to fostering democratic deliberation has received little attention. Drawing on a national panel survey, this study explores the cognitive processes by which social media news users learn new perspectives from disagreement via the mediating role of interpersonal discussion and the moderating role of fact-checking. The results reveal that social media news use is positively associated with learning from disagreement, and this relationship is mediated by like-minded and cross-cutting discussions. Specifically, while cross-cutting discussions facilitate individuals' understanding and learning of opposing viewpoints, like-minded discussions hinder this process. Furthermore, fact-checking has the potential to reinforce not only tolerant attitudes toward opposing viewpoints but also individuals’ pre-existing beliefs and impede learning from disagreement. The mixed effect of fact-checking depends on the type of interpersonal discussion taking place. In light of these findings, we discuss the contributions and shortcomings of SNSs and fact-checking behavior in terms of the ideals of deliberative democracy.}
}

﻿@article{jehn1994enhancing,
  author    = {Jehn, Karen A.},
  title     = {ENHANCING EFFECTIVENESS: AN INVESTIGATION OF ADVANTAGES AND DISADVANTAGES OF VALUE‐BASED INTRAGROUP CONFLICT},
  journal   = {International Journal of Conflict Management},
  year      = {1994},
  month     = {Jan},
  day       = {01},
  publisher = {MCB UP Ltd},
  volume    = {5},
  number    = {3},
  pages     = {223-238},
  abstract  = {This study investigates the benefits and detriments of emotional and task‐related conflict in work groups. Group value consensus (GVC), or the extent to which group members share values, and group value fit (GVF), or the degree to which the culture of the group matches the ideal culture envisioned by external parties with control over the group, are hypothesized to decrease conflict. In examining 88 workgroups performing comparable organizational tasks, it was found that groups with low levels of value similarity among members and between the group and governing superiors had higher levels of conflict than groups with high levels of value similarity. As hypothesized, emotional conflict was negatively associated with group performance and satisfaction, while task conflict was positively associated with group performance. The implications of these results for conflict management and group effectiveness are discussed.},
  issn      = {1044-4068},
  doi       = {10.1108/eb022744},
  url       = {https://doi.org/10.1108/eb022744}
}



@article{beck2019managing,
  title   = {Managing conflict in online debate communities},
  volume  = {24},
  url     = {https://firstmonday.org/ojs/index.php/fm/article/view/9585},
  doi     = {10.5210/fm.v24i7.9585},
  number  = {7},
  journal = {First Monday},
  author  = {Beck,
             Jordan and Neupane,
             Bikalpa and Carroll,
             John M.},
  year    = {2019},
  month   = {Jun.}
}

@article{stromer2009agreement,
  author    = {Jennifer Stromer-Galley and Peter Muhlberger},
  title     = {Agreement and Disagreement in Group Deliberation: Effects on Deliberation Satisfaction, Future Engagement, and Decision Legitimacy},
  journal   = {Political Communication},
  volume    = {26},
  number    = {2},
  pages     = {173-192},
  year      = {2009},
  publisher = {Routledge},
  doi       = {10.1080/10584600902850775},
  url       = { https://doi.org/10.1080/10584600902850775 },
  eprint    = { https://doi.org/10.1080/10584600902850775 }
}

@article{lourie2021scruples,
  title        = {SCRUPLES: A Corpus of Community Ethical Judgments on 32,000 Real-Life Anecdotes},
  volume       = {35},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/17589},
  doi          = {10.1609/aaai.v35i15.17589},
  abstractnote = {As AI systems become an increasing part of people’s everyday lives, it becomes ever more important that they understand people’s ethical norms. Motivated by descriptive ethics, a field of study that focuses on people’s descriptive judgments rather than theoretical prescriptions on morality, we investigate a novel, data-driven approach to machine ethics. We introduce SCRUPLES, the first large-scale dataset with 625,000 ethical judgments over 32,000 real-life anecdotes. Each anecdote recounts a complex ethical situation, often posing moral dilemmas, paired with a distribution of judgments contributed by the community members. Our dataset presents a major challenge to state-of-the-art neural language models, leaving significant room for improvement. However, when presented with simplified moral situations, the results are considerably more promising, suggesting that neural models can effectively learn simpler ethical building blocks. A key take-away of our empirical analysis is that norms are not always clean-cut; many situations are naturally divisive. We present a new method to estimate the best possible performance on such tasks with inherently diverse label distributions, and explore likelihood functions that separate intrinsic from model uncertainty. Data and code are available at https://github.com/allenai/scruples.},
  number       = {15},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  author       = {Lourie, Nicholas and Le Bras, Ronan and Choi, Yejin},
  year         = {2021},
  month        = {May},
  pages        = {13470-13479}
}


@article{schwartz2021repository,
  author  = {Schwartz, Shalom},
  year    = {2021},
  month   = {09},
  pages   = {},
  title   = {A Repository of Schwartz Value Scales with Instructions and an Introduction},
  volume  = {2},
  journal = {Online Readings in Psychology and Culture},
  doi     = {10.9707/2307-0919.1173}
}

@article{schwartz2012refining,
  title     = {Refining the theory of basic individual values.},
  author    = {Schwartz, Shalom H and Cieciuch, Jan and Vecchione, Michele and Davidov, Eldad and Fischer, Ronald and Beierlein, Constanze and Ramos, Alice and Verkasalo, Markku and L{\"o}nnqvist, Jan-Erik and Demirutku, Kursad and others},
  journal   = {Journal of personality and social psychology},
  volume    = {103},
  doi       = {https://doi.org/10.1037/a0029393},
  number    = {4},
  pages     = {663},
  year      = {2012},
  publisher = {American Psychological Association}
}

@article{kass1995bayes,
  author    = {Robert E. Kass and Adrian E. Raftery},
  title     = {Bayes Factors},
  journal   = {Journal of the American Statistical Association},
  volume    = {90},
  number    = {430},
  pages     = {773-795},
  year      = {1995},
  publisher = {Taylor & Francis},
  doi       = {10.1080/01621459.1995.10476572},
  url       = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1995.10476572},
  eprint    = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.1995.10476572}
}


@article{vallat2018pingouin,
  doi       = {10.21105/joss.01026},
  url       = {https://doi.org/10.21105/joss.01026},
  year      = {2018},
  publisher = {The Open Journal},
  volume    = {3},
  number    = {31},
  pages     = {1026},
  author    = {Raphael Vallat},
  title     = {Pingouin: statistics in Python},
  journal   = {Journal of Open Source Software}
}


@article{rouder2009bayesian,
  author   = {Rouder, Jeffrey N.
              and Speckman, Paul L.
              and Sun, Dongchu
              and Morey, Richard D.
              and Iverson, Geoffrey},
  title    = {Bayesian t tests for accepting and rejecting the null hypothesis},
  journal  = {Psychonomic Bulletin {\&} Review},
  year     = {2009},
  month    = {Apr},
  day      = {01},
  volume   = {16},
  number   = {2},
  pages    = {225--237},
  abstract = {Progress in science often comes from discovering invariances in relationships among variables; these invariances often correspond to null hypotheses. As is commonly known, it is not possible to state evidence for the null hypothesis in conventional significance testing. Here we highlight a Bayes factor alternative to the conventional t test that will allow researchers to express preference for either the null hypothesis or the alternative. The Bayes factor has a natural and straightforward interpretation, is based on reasonable assumptions, and has better properties than other methods of inference that have been advocated in the psychological literature. To facilitate use of the Bayes factor, we provide an easy-to-use, Web-based program that performs the necessary calculations.},
  issn     = {1531-5320},
  doi      = {10.3758/PBR.16.2.225},
  url      = {https://doi.org/10.3758/PBR.16.2.225}
}


@inproceedings{wolf2020transformers,
  title     = {Transformers: State-of-the-Art Natural Language Processing},
  author    = {Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  month     = oct,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://www.aclweb.org/anthology/2020.emnlp-demos.6},
  pages     = {38--45}
}

@article{scikit-learn,
  author  = {Fabian Pedregosa and Ga{{\"e}}l Varoquaux and Alexandre Gramfort and Vincent Michel and Bertrand Thirion and Olivier Grisel and Mathieu Blondel and Peter Prettenhofer and Ron Weiss and Vincent Dubourg and Jake Vanderplas and Alexandre Passos and David Cournapeau and Matthieu Brucher and Matthieu Perrot and {{\'E}}douard Duchesnay},
  title   = {Scikit-learn: Machine Learning in Python},
  journal = {Journal of Machine Learning Research},
  year    = {2011},
  volume  = {12},
  number  = {85},
  pages   = {2825--2830},
  url     = {http://jmlr.org/papers/v12/pedregosa11a.html}
}

@article{mehrabi2021survey,
  author     = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
  title      = {A Survey on Bias and Fairness in Machine Learning},
  year       = {2021},
  issue_date = {July 2022},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {54},
  number     = {6},
  issn       = {0360-0300},
  url        = {https://doi.org/10.1145/3457607},
  doi        = {10.1145/3457607},
  abstract   = {With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.},
  journal    = {ACM Comput. Surv.},
  month      = {jul},
  articleno  = {115},
  numpages   = {35},
  keywords   = {Fairness and bias in artificial intelligence, machine learning, representation learning, deep learning, natural language processing}
}


@article{proferes2021studying,
  author   = {Nicholas Proferes and Naiyan Jones and Sarah Gilbert and Casey Fiesler and Michael Zimmer},
  title    = {Studying {R}eddit: A Systematic Overview of Disciplines, Approaches, Methods, and Ethics},
  journal  = {Social Media + Society},
  volume   = {7},
  number   = {2},
  pages    = {20563051211019004},
  year     = {2021},
  doi      = {10.1177/20563051211019004},
  url      = {https://doi.org/10.1177/20563051211019004},
  eprint   = {https://doi.org/10.1177/20563051211019004},
  abstract = { This article offers a systematic analysis of 727 manuscripts that used Reddit as a data source, published between 2010 and 2020. Our analysis reveals the increasing growth in use of Reddit as a data source, the range of disciplines this research is occurring in, how researchers are getting access to Reddit data, the characteristics of the datasets researchers are using, the subreddits and topics being studied, the kinds of analysis and methods researchers are engaging in, and the emerging ethical questions of research in this space. We discuss how researchers need to consider the impact of Reddit’s algorithms, affordances, and generalizability of the scientific knowledge produced using Reddit data, as well as the potential ethical dimensions of research that draws data from subreddits with potentially sensitive populations. }
}


 @misc{prodigy_montani_honnibal,
  title     = {Prodigy: A modern and scriptable annotation tool for creating training data for machine learning models},
  url       = {https://prodi.gy/},
  journal   = {Prodigy},
  year      = {2022},
  publisher = {Explosion},
  author    = {Montani, Ines and Honnibal, Matthew}
}


@article{jhaver2019does,
  author     = {Jhaver, Shagun and Bruckman, Amy and Gilbert, Eric},
  title      = {Does Transparency in Moderation Really Matter? User Behavior After Content Removal Explanations on Reddit},
  year       = {2019},
  issue_date = {November 2019},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {3},
  number     = {CSCW},
  url        = {https://doi.org/10.1145/3359252},
  doi        = {10.1145/3359252},
  abstract   = {When posts are removed on a social media platform, users may or may not receive an explanation. What kinds of explanations are provided? Do those explanations matter? Using a sample of 32 million Reddit posts, we characterize the removal explanations that are provided to Redditors, and link them to measures of subsequent user behaviors---including future post submissions and future post removals. Adopting a topic modeling approach, we show that removal explanations often provide information that educate users about the social norms of the community, thereby (theoretically) preparing them to become a productive member. We build regression models that show evidence of removal explanations playing a role in future user activity. Most importantly, we show that offering explanations for content moderation reduces the odds of future post removals. Additionally, explanations provided by human moderators did not have a significant advantage over explanations provided by bots for reducing future post removals. We propose design solutions that can promote the efficient use of explanation mechanisms, reflecting on how automated moderation tools can contribute to this space. Overall, our findings suggest that removal explanations may be under-utilized in moderation practices, and it is potentially worthwhile for community managers to invest time and resources into providing them.},
  journal    = {Proc. ACM Hum.-Comput. Interact.},
  month      = {nov},
  articleno  = {150},
  numpages   = {27},
  keywords   = {platform governance, content moderation, post removals, content regulation}
}


@article{chew2021predicting,
  author   = {Chew, Robert
              and Kery, Caroline
              and Baum, Laura
              and Bukowski, Thomas
              and Kim, Annice
              and Navarro, Mario},
  title    = {Predicting Age Groups of Reddit Users Based on Posting Behavior and Metadata: Classification Model Development and Validation},
  journal  = {JMIR Public Health Surveill},
  year     = {2021},
  month    = {Mar},
  day      = {16},
  volume   = {7},
  number   = {3},
  pages    = {e25807},
  keywords = {Reddit; social media; age; machine learning; classification},
  abstract = {Background: Social media are important for monitoring perceptions of public health issues and for educating target audiences about health; however, limited information about the demographics of social media users makes it challenging to identify conversations among target audiences and limits how well social media can be used for public health surveillance and education outreach efforts. Certain social media platforms provide demographic information on followers of a user account, if given, but they are not always disclosed, and researchers have developed machine learning algorithms to predict social media users' demographic characteristics, mainly for Twitter. To date, there has been limited research on predicting the demographic characteristics of Reddit users. Objective: We aimed to develop a machine learning algorithm that predicts the age segment of Reddit users, as either adolescents or adults, based on publicly available data. Methods: This study was conducted between January and September 2020 using publicly available Reddit posts as input data. We manually labeled Reddit users' age by identifying and reviewing public posts in which Reddit users self-reported their age. We then collected sample posts, comments, and metadata for the labeled user accounts and created variables to capture linguistic patterns, posting behavior, and account details that would distinguish the adolescent age group (aged 13 to 20 years) from the adult age group (aged 21 to 54 years). We split the data into training (n=1660) and test sets (n=415) and performed 5-fold cross validation on the training set to select hyperparameters and perform feature selection. We ran multiple classification algorithms and tested the performance of the models (precision, recall, F1 score) in predicting the age segments of the users in the labeled data. To evaluate associations between each feature and the outcome, we calculated means and confidence intervals and compared the two age groups, with 2-sample t tests, for each transformed model feature. Results: The gradient boosted trees classifier performed the best, with an F1 score of 0.78. The test set precision and recall scores were 0.79 and 0.89, respectively, for the adolescent group (n=254) and 0.78 and 0.63, respectively, for the adult group (n=161). The most important feature in the model was the number of sentences per comment (permutation score: mean 0.100, SD 0.004). Members of the adolescent age group tended to have created accounts more recently, have higher proportions of submissions and comments in the r/teenagers subreddit, and post more in subreddits with higher subscriber counts than those in the adult group. Conclusions: We created a Reddit age prediction algorithm with competitive accuracy using publicly available data, suggesting machine learning methods can help public health agencies identify age-related target audiences on Reddit. Our results also suggest that there are characteristics of Reddit users' posting behavior, linguistic patterns, and account features that distinguish adolescents from adults. },
  issn     = {2369-2960},
  doi      = {10.2196/25807},
  url      = {https://publichealth.jmir.org/2021/3/e25807}
}


@inproceedings{ghosh2021laughing,
  title     = {{``}Laughing at you or with you{''}: The Role of Sarcasm in Shaping the Disagreement Space},
  author    = {Ghosh, Debanjan  and
               Shrivastava, Ritvik  and
               Muresan, Smaranda},
  booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
  month     = apr,
  year      = {2021},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2021.eacl-main.171},
  doi       = {10.18653/v1/2021.eacl-main.171},
  pages     = {1998--2010},
  abstract  = {Detecting arguments in online interactions is useful to understand how conflicts arise and get resolved. Users often use figurative language, such as sarcasm, either as persuasive devices or to attack the opponent by an ad hominem argument. To further our understanding of the role of sarcasm in shaping the disagreement space, we present a thorough experimental setup using a corpus annotated with both argumentative moves (agree/disagree) and sarcasm. We exploit joint modeling in terms of (a) applying discrete features that are useful in detecting sarcasm to the task of argumentative relation classification (agree/disagree/none), and (b) multitask learning for argumentative relation classification and sarcasm detection using deep learning architectures (e.g., dual Long Short-Term Memory (LSTM) with hierarchical attention and Transformer-based architectures). We demonstrate that modeling sarcasm improves the argumentative relation classification task (agree/disagree/none) in all setups.}
}

@inproceedings{joulin2017bag,
  title     = {Bag of Tricks for Efficient Text Classification},
  author    = {Joulin, Armand  and
               Grave, Edouard  and
               Bojanowski, Piotr  and
               Mikolov, Tomas},
  booktitle = {Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers},
  month     = apr,
  year      = {2017},
  address   = {Valencia, Spain},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/E17-2068},
  pages     = {427--431},
  abstract  = {This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore CPU, and classify half a million sentences among 312K classes in less than a minute.}
}

@inproceedings{luo2023improving,
  author    = {Luo, Yun and Liu, Zihan and Li, Stan Z. and Zhang, Yue},
  title     = {Improving (Dis)Agreement Detection with Inductive Social Relation Information From Comment-Reply Interactions},
  year      = {2023},
  isbn      = {9781450394161},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3543507.3583314},
  doi       = {10.1145/3543507.3583314},
  abstract  = {(Dis)agreement detection aims to identify the authors’ attitudes or positions (agree, disagree, neutral) towards a specific text. It is limited for existing methods merely using textual information for identifying (dis)agreements, especially for cross-domain settings. Social relation information can play an assistant role in the (dis)agreement task besides textual information. We propose a novel method to extract such relation information from (dis)agreement data into an inductive social relation graph, merely using the comment-reply pairs without any additional platform-specific information. The inductive social relation globally considers the historical discussion and the relation between authors. Textual information based on a pre-trained language model and social relation information encoded by pre-trained RGCN are jointly considered for (dis)agreement detection. Experimental results show that our model achieves state-of-the-art performance for both the in-domain and cross-domain tasks on the benchmark – DEBAGREEMENT. We find social relations can boost the performance of the (dis)agreement detection model, especially for the long-token comment-reply pairs, demonstrating the effectiveness of the social relation graph. We also explore the effect of the knowledge graph embedding methods, the information fusing method, and the time interval in constructing the social relation graph, which shows the effectiveness of our model.},
  booktitle = {Proceedings of the ACM Web Conference 2023},
  pages     = {1584–1593},
  numpages  = {10},
  keywords  = {Social Relation, Stance Detection, Disagreement Detection, Opinion Mining},
  location  = {Austin, TX, USA},
  series    = {WWW '23}
}


@article{moerbeek2021bayesian,
  author   = {Moerbeek, Mirjam},
  title    = {Bayesian updating: increasing sample size during the course of a study},
  journal  = {BMC Medical Research Methodology},
  year     = {2021},
  month    = {Jul},
  day      = {05},
  volume   = {21},
  number   = {1},
  pages    = {137},
  abstract = {A priori sample size calculation requires an a priori estimate of the size of the effect. An incorrect estimate may result in a sample size that is too low to detect effects or that is unnecessarily high. An alternative to a priori sample size calculation is Bayesian updating, a procedure that allows increasing sample size during the course of a study until sufficient support for a hypothesis is achieved. This procedure does not require and a priori estimate of the effect size. This paper introduces Bayesian updating to researchers in the biomedical field and presents a simulation study that gives insight in sample sizes that may be expected for two-group comparisons.},
  issn     = {1471-2288},
  doi      = {10.1186/s12874-021-01334-6},
  url      = {https://doi.org/10.1186/s12874-021-01334-6}
}

@article{croux2010influence,
  author   = {Croux, Christophe
              and Dehon, Catherine},
  title    = {Influence functions of the Spearman and Kendall correlation measures},
  journal  = {Statistical Methods {\&} Applications},
  year     = {2010},
  month    = {Nov},
  day      = {01},
  volume   = {19},
  number   = {4},
  pages    = {497--515},
  abstract = {Nonparametric correlation estimators as the Kendall and Spearman correlation are widely used in the applied sciences. They are often said to be robust, in the sense of being resistant to outlying observations. In this paper we formally study their robustness by means of their influence functions and gross-error sensitivities. Since robustness of an estimator often comes at the price of an increased variance, we also compute statistical efficiencies at the normal model. We conclude that both the Spearman and Kendall correlation estimators combine a bounded and smooth influence function with a high efficiency. In a simulation experiment we compare these nonparametric estimators with correlations based on a robust covariance matrix estimator.},
  issn     = {1613-981X},
  doi      = {10.1007/s10260-010-0142-z},
  url      = {https://doi.org/10.1007/s10260-010-0142-z}
}


@article{kruschke2018rejecting,
  author   = {John K. Kruschke},
  title    = {Rejecting or Accepting Parameter Values in Bayesian Estimation},
  journal  = {Advances in Methods and Practices in Psychological Science},
  volume   = {1},
  number   = {2},
  pages    = {270-280},
  year     = {2018},
  doi      = {10.1177/2515245918771304},
  url      = {https://doi.org/10.1177/2515245918771304},
  eprint   = {https://doi.org/10.1177/2515245918771304},
  abstract = { This article explains a decision rule that uses Bayesian posterior distributions as the basis for accepting or rejecting null values of parameters. This decision rule focuses on the range of plausible values indicated by the highest density interval of the posterior distribution and the relation between this range and a region of practical equivalence (ROPE) around the null value. The article also discusses considerations for setting the limits of a ROPE and emphasizes that analogous considerations apply to setting the decision thresholds for p values and Bayes factors. }
}



@inproceedings{kiesel2023semeval,
  title     = {{S}em{E}val-2023 Task 4: {V}alue{E}val: Identification of Human Values Behind Arguments},
  author    = {Kiesel, Johannes  and
               Alshomary, Milad  and
               Mirzakhmedova, Nailia  and
               Heinrich, Maximilian  and
               Handke, Nicolas  and
               Wachsmuth, Henning  and
               Stein, Benno},
  booktitle = {Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)},
  month     = jul,
  year      = {2023},
  address   = {Toronto, Canada},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.semeval-1.313},
  doi       = {10.18653/v1/2023.semeval-1.313},
  pages     = {2287--2303},
  abstract  = {Argumentation is ubiquitous in natural language communication, from politics and media to everyday work and private life. Many arguments derive their persuasive power from human values, such as self-directed thought or tolerance, albeit often implicitly. These values are key to understanding the semantics of arguments, as they are generally accepted as justifications for why a particular option is ethically desirable. Can automated systems uncover the values on which an argument draws? To answer this question, 39 teams submitted runs to ValueEval{'}23. Using a multi-sourced dataset of over 9K arguments, the systems achieved F1-scores up to 0.87 (nature) and over 0.70 for three more of 20 universal value categories. However, many challenges remain, as evidenced by the low peak F1-score of 0.39 for stimulation, hedonism, face, and humility.}
}


@article{van2022advantages,
  doi       = {10.1037/met0000415},
  url       = {https://doi.org/10.1037/met0000415},
  year      = {2022},
  month     = jun,
  publisher = {American Psychological Association ({APA})},
  volume    = {27},
  number    = {3},
  pages     = {451--465},
  author    = {Don van Ravenzwaaij and Eric-Jan Wagenmakers},
  title     = {Advantages masquerading as {\textquotedblleft}issues{\textquotedblright} in Bayesian hypothesis testing: A commentary on Tendeiro and Kiers (2019).},
  journal   = {Psychological Methods}
}

@inproceedings{liscio2023does,
  title     = {What does a Text Classifier Learn about Morality? An Explainable Method for Cross-Domain Comparison of Moral Rhetoric},
  author    = {Liscio, Enrico  and
               Araque, Oscar  and
               Gatti, Lorenzo  and
               Constantinescu, Ionut  and
               Jonker, Catholijn  and
               Kalimeri, Kyriaki  and
               Murukannaiah, Pradeep Kumar},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = jul,
  year      = {2023},
  address   = {Toronto, Canada},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.acl-long.789},
  doi       = {10.18653/v1/2023.acl-long.789},
  pages     = {14113--14132},
  abstract  = {Moral rhetoric influences our judgement. Although social scientists recognize moral expression as domain specific, there are no systematic methods for analyzing whether a text classifier learns the domain-specific expression of moral language or not. We propose Tomea, a method to compare a supervised classifier{'}s representation of moral rhetoric across domains. Tomea enables quantitative and qualitative comparisons of moral rhetoric via an interpretable exploration of similarities and differences across moral concepts and domains. We apply Tomea on moral narratives in thirty-five thousand tweets from seven domains. We extensively evaluate the method via a crowd study, a series of cross-domain moral classification comparisons, and a qualitative analysis of cross-domain moral expression.}
}


@inproceedings{fang2019neural,
  title     = {Neural Multi-Task Learning for Stance Prediction},
  author    = {Fang, Wei  and
               Nadeem, Moin  and
               Mohtarami, Mitra  and
               Glass, James},
  booktitle = {Proceedings of the Second Workshop on Fact Extraction and VERification (FEVER)},
  month     = nov,
  year      = {2019},
  address   = {Hong Kong, China},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/D19-6603},
  doi       = {10.18653/v1/D19-6603},
  pages     = {13--19},
  abstract  = {We present a multi-task learning model that leverages large amount of textual information from existing datasets to improve stance prediction. In particular, we utilize multiple NLP tasks under both unsupervised and supervised settings for the target stance prediction task. Our model obtains state-of-the-art performance on a public benchmark dataset, Fake News Challenge, outperforming current approaches by a wide margin.}
}

@misc{vandermeer2023do_data,
  title     = {Do Differences in Values Influence Disagreements in Online Discussions?},
  url       = {osf.io/42dns},
  doi       = {10.17605/OSF.IO/42DNS},
  publisher = {OSF},
  author    = {van der Meer, Michiel},
  year      = {2023},
  month     = {Dec}
}