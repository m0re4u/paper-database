% Encoding: UTF-8

@InProceedings{cabrio2018five,
  author        = {Cabrio, Elena and Villata, Serena},
  title         = {Five Years of Argument Mining: a Data-driven Analysis.},
  booktitle     = {IJCAI},
  year          = {2018},
  volume        = {18},
  pages         = {5427--5433},
  __markedentry = {[m0re:1]},
  keywords      = {rank4},
  review        = {Overview paper of Argument Mining.
Good overview of datasets!2},
}

@InProceedings{levy2014context,
  author    = {Levy, Ran and Bilu, Yonatan and Hershcovich, Daniel and Aharoni, Ehud and Slonim, Noam},
  title     = {Context dependent claim detection},
  booktitle = {Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers},
  year      = {2014},
  pages     = {1489--1500},
}

@InProceedings{bar-haim2017stance,
  author    = {Bar-Haim, Roy and Bhattacharya, Indrajit and Dinuzzo, Francesco and Saha, Amrita and Slonim, Noam},
  title     = {Stance classification of context-dependent claims},
  booktitle = {roceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long PapersPr},
  year      = {2017},
  pages     = {251--261},
}

@Article{eger2017neural,
  author        = {Eger, Steffen and Daxenberger, Johannes and Gurevych, Iryna},
  title         = {Neural end-to-end learning for computational argumentation mining},
  journal       = {arXiv preprint arXiv:1704.06104},
  year          = {2017},
  __markedentry = {[m0re:]},
}

@InProceedings{duthie2018deep,
  author        = {Duthie, Rory and Budzynska, Katarzyna},
  title         = {A Deep Modular RNN Approach for Ethos Mining.},
  booktitle     = {IJCAI},
  year          = {2018},
  pages         = {4041--4047},
  __markedentry = {[m0re:]},
}

@Article{schulz2018multi,
  author      = {Claudia Schulz and Steffen Eger and Johannes Daxenberger and Tobias Kahse and Iryna Gurevych},
  title       = {Multi-Task Learning for Argumentation Mining in Low-Resource Settings},
  abstract    = {We investigate whether and where multi-task learning (MTL) can improve performance on NLP problems related to argumentation mining (AM), in particular argument component identification. Our results show that MTL performs particularly well (and better than single-task learning) when little training data is available for the main task, a common scenario in AM. Our findings challenge previous assumptions that conceptualizations across AM datasets are divergent and that MTL is difficult for semantic or higher-level tasks.},
  date        = {2018-04-11},
  eprint      = {1804.04083v3},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1804.04083v3:PDF},
  keywords    = {cs.CL},
}

@InProceedings{jo2019cascade,
  author       = {Jo, Yohan and Visser, Jacky and Reed, Chris and Hovy, Eduard},
  title        = {A cascade model for proposition extraction in argumentation},
  booktitle    = {Proceedings of the 6th Workshop on Argument Mining},
  year         = {2019},
  pages        = {11--24},
  organization = {Association for Computational Linguistics},
}

@Article{lawrence2020argument,
  author    = {Lawrence, John and Reed, Chris},
  title     = {Argument mining: A survey},
  journal   = {Computational Linguistics},
  year      = {2020},
  volume    = {45},
  number    = {4},
  pages     = {765--818},
  keywords  = {rank4},
  publisher = {MIT Press},
}

@Article{peldszus2013argument,
  author    = {Peldszus, Andreas and Stede, Manfred},
  title     = {From argument diagrams to argumentation mining in texts: A survey},
  journal   = {International Journal of Cognitive Informatics and Natural Intelligence (IJCINI)},
  year      = {2013},
  volume    = {7},
  number    = {1},
  pages     = {1--31},
  publisher = {IGI Global},
}

@Article{steenbergen2003measuring,
  author    = {Steenbergen, Marco R and B{\"a}chtiger, Andr{\'e} and Sp{\"o}rndli, Markus and Steiner, J{\"u}rg},
  title     = {Measuring political deliberation: A discourse quality index},
  journal   = {Comparative European Politics},
  year      = {2003},
  volume    = {1},
  number    = {1},
  pages     = {21--48},
  keywords  = {rank3},
  publisher = {Springer},
}

@Article{klein2012enabling,
  author    = {Klein, Mark},
  title     = {Enabling large-scale deliberation using attention-mediation metrics},
  journal   = {Computer Supported Cooperative Work (CSCW)},
  year      = {2012},
  volume    = {21},
  number    = {4-5},
  pages     = {449--473},
  publisher = {Springer},
}

@Article{reimers2019classification,
  author  = {Reimers, Nils and Schiller, Benjamin and Beck, Tilman and Daxenberger, Johannes and Stab, Christian and Gurevych, Iryna},
  title   = {Classification and clustering of arguments with contextualized word embeddings},
  journal = {arXiv preprint arXiv:1906.09821},
  year    = {2019},
}

@Article{spliethoever2019is,
  author      = {Maximilian Splieth√∂ver and Jonas Klaff and Hendrik Heuer},
  title       = {Is It Worth the Attention? A Comparative Evaluation of Attention Layers for Argument Unit Segmentation},
  abstract    = {Attention mechanisms have seen some success for natural language processing downstream tasks in recent years and generated new State-of-the-Art results. A thorough evaluation of the attention mechanism for the task of Argumentation Mining is missing, though. With this paper, we report a comparative evaluation of attention layers in combination with a bidirectional long short-term memory network, which is the current state-of-the-art approach to the unit segmentation task. We also compare sentence-level contextualized word embeddings to pre-generated ones. Our findings suggest that for this task the additional attention layer does not improve upon a less complex approach. In most cases, the contextualized embeddings do also not show an improvement on the baseline score.},
  date        = {2019-06-24},
  eprint      = {1906.10068v1},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {online:http\://arxiv.org/pdf/1906.10068v1:PDF},
  keywords    = {cs.CL, cs.LG},
}

@Comment{jabref-meta: databaseType:bibtex;}
